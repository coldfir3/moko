---
title: "Multiobjective Optimization of the Nowacki Beam"
author: "Adriano G. Passos"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: nb_vignette.bib
vignette: >
  %\VignetteIndexEntry{Multiobjective Optimization of the Nowacki Beam}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

In this paper, the well known multiobjective optimization problem: The Nowacki beam is solved. here, three different frameworks for dealing with many-objective problems using Kriging surrogate models are compared:

  1. The efficient global optimization (EGO) algorithm applied to a single objective function of the combined objectives responses (MEGO);
  
  1. The iterative maximization of the expected hypervolume improvement (EHVI);
  
  1. A novel approach is also proposed here, the variance minimization of the Kriging-predicted Pareto front (VMKF). 

To evaluate the efficiency of these three methods, a baseline solution is created by multiobjective direct optimization (no surrogates are used) applying the NSGA-II algorithm.

## Introduction

Multiobjective optimization is a field of interest for many real-world applications. Usually, projects have multiple and conflicting goals and, most of the time, the relationship between the decision space (design variables) and the outcome is highly complex. 

In the past years, Kriging have become one of the most popular surrogates on the industry [@forrester2009recent]. When using Kriging, usually the efficient global optimization (EGO) algorithm is the standard technique for single objective optimization. For costly multiple objectives, direct combination of the Kriging predictions and a multiobjective genetic algorithm (MOGA) can be used such as in [@li2011improved]. However, according to [@forrester2009recent; @forrester2008engineering], there are currently two popular ways of constructing Pareto sets. The first approach, is to combine all goals into a single quantity and carry out the EGO algorithm. The weighting function have adjustable parameters that changes during the optimization problem so that the algorithm can potentially sweep all the objective space. For simplicity, here, this approach will be simply called MEGO. Another popular way to address many-objective problems is to generalize the expected improvement (EI) criterion into what is called the expected hypervolume improvement (EHVI) [@emmerich2011hypervolume; @shimoyama2013kriging]. Although there are some efficient algorithms to calculate and/or estimate the expected hypervolume improvement such as [@hupkens2014faster], it is usually a costly operation which significantly scales with the size of the Pareto set. To overcome this issue, a simpler, yet robust, algorithm is proposed by the present work. Here, each goal is modeled using Kriging then a state-of-the-art multiobjective algorithm (NSGA-II) is used to generate a Pareto set of the predicted mean of the surrogate models. From them, the design with higher value of predicted variance is chosen as an infill point.

## Surrogate Multiobjetive Approaches

In the current work, three different Kriging-based multiobjective frameworks are studied, which are discussed in the following subsections. The derivation of the Kriging predictor and the design of experiments (DOE) concept are not covered in this paper. The reader can find a comprehensive mathematical description of these subjects in [@forrester2008engineering]. The Kriging models where built using the `R` package `DiceKriging` [@roustant2012dicekriging].


### Multiobjective Efficient Global Optimization (MEGO)

EGO, proposed by Jones [@jones1998efficient] for mono-objective optimization, consists in, from an initial set of samples $\mathbf{X}$, a Kriging model is built using the responses of a high-fidelity model, then the algorithm sequentially maximizes the expected improvement (EI) and updates the model at each iteration (including re-estimation of the hyperparameters).

The basic idea of the EI criterion is that by sampling a new point $\mathbf{x^\star}$ the results will be improved by $y_\text{min} - y(\mathbf{x^\star})$ if $y(\mathbf{x^\star}) < y_\text{min}$ or $0$ otherwise, where $y_\text{min}$ is the lowest value of $\mathbf{y}$ so far. Obviously, the value of this improvement is not known in advance because $y(\mathbf{x^\star})$ is unknown. However, the expectation of this improvement can be obtained using the information from the Kriging predictor. The EI criterion has important properties for sequential exploitation and exploration (filling) of the design space: it is null at points already visited (thus preventing searches in well-known regions and increasing the possibility of convergence); and at all other points it is positive and its magnitude increases with predicted variance (favoring searches in unexplored regions) and decreases with the predicted mean (favoring searches in regions with low predicted values).

The EGO algorithm can be easily imported to a multiobjective framework by creating a combined function of the qualities [@knowles2006parego]. The constrains of the optimization problem can be considered simply by building independent metamodels for each constraint and multiplying the EI of the composed objective function by the probability of each constraint to be met [@sasena2002exploration]. The MEGO algorithm can be summarized as follows:

  1. Generate an initial DOE $\mathbf{X}$ using an optimized *Latin hypercube*;
	
  1. Evaluate $\mathbf{X}$ using high-fidelity models and store responses of $\mathbf{f}=f_{(1 \ldots m)}(\mathbf{x})$ and $\mathbf{g}=g_{(1 \ldots p)}(\mathbf{x})$ for the $m$-objectives and $p$-constraints.
	1. **while** computational budget not exhausted **do**:
    a. Normalize the responses to fit in a hypercube of size $[0,1]^m$;
    a. For each $\mathbf{x} \in \mathbf{X}$ Compute a scalar quality by making $f_{\mathbf{\lambda}} = \underset{i=1}{\overset{m}{\text{max}}}(\lambda_i f_i) + \rho \sum^m_{(i=1)}\lambda_i f_i$, where $\mathbf{\lambda}$ is drawn uniformly at random from a set of evenly distributed unit vectors and $\rho$ is an arbitrary small value which we set to $0.05$;
    a. Build Kriging models for $f_{\mathbf{\lambda}}$ and for the constraints $\mathbf{g}$;
    a. Find $\mathbf{x}^\star$ that maximizes the *constrained expected improvement*: $\mathbf{x}^\star = \text{arg}(\text{max}(\text{EI}_C(\mathbf{x})))$;
	  a. Evaluate the ``true'' values of $f(\mathbf{x}^\star$) and $g(\mathbf{x}^\star)$ using high-fidelity models and update the database.
  1. **end while**.

Here, the EI is computed using a custom modified version of the functions provided on the `R` package `DiceOptim` [@diceoptim] so that it could handle constrains. Also, on all approaches, the optimized Latin hypercube is built using the `R` package `lhs` [@R.LHS].

### Expected Hypervolume Improvement (EHVI)

For comparison, the expected hypervolume improvement (EHVI) is used as infill criterion. The EHVI is based on the theory of the hypervolume indicator [@zitzler1998multiobjective], a metric of dominance of non-dominated solutions have. This metric consists in the size of the hypervolume fronted by the non-dominated set bounded by reference maximum points. I that sense, the EHVI is the expected improvement at the hypervolume size we would get by sampling a new point $\mathbf{x^\star}$. Here, the EHVI is computed using the \R package \texttt{GPareto} [@GPareto]. The EHVI function provided by this package do not account for constrains so a custom modification had to be implemented. The algorithm used here is similar to the MEGO and can be summarized as follows:

  1. Generate an initial DOE $\mathbf{X}$ using an optimized *Latin hypercube*;
  1. Evaluate $\mathbf{X}$ using high-fidelity models and store responses of $\mathbf{f}=f_{(1 \ldots m)}$ and $\mathbf{g}=g_{(1 \ldots p)}$ for the $m$-objectives and $p$-constraints.
  1. **while** computational budget not exhausted **do**:
    a. Normalize the responses to fit a hypercube of size $[0,1]^m$;
    a. For each of the $m$-objectives and $p$-constraints, build a Kriging model;
    a. Find $\mathbf{x}^\star$ that maximizes the *constrained expected hypervolume improvement*: $\mathbf{x}^\star = \text{arg}(\text{max}(\text{EHVI}_C(\mathbf{x})))$;
    a. Evaluate the ``true'' values of $f(\mathbf{x}^\star$) and $g(\mathbf{x}^\star)$ using high-fidelity models and update the database.
  1. **end while**.


For this and the previous approach, the algorithm used to maximize the infill criteria ($\text{EHVI}_C$ and $\text{EI}_C$, respectively) is the one provided by the `R` package `GenSA` [@GenSA] which stands for generalized simulated annealing.

### Variance Minimization of the Kriging-predicted Front (VMKF)

The proposed framework, VMKF, is based on the iterative improvement of the predicted Pareto set fidelity. Here, the idea is, from a given initial set of Kriging models (one for each cost or constraint function), to build a Pareto front using the predictor's mean of each model as input functions. From the estimated front $\mathbf{P}$, the design with higher variance $\mathbf{x}^\star$ (i.e.: most isolated on the decision space) have it's "true" value evaluated using the high fidelity models. A new set of Kriging models are then rebuilt and the process repeats until a stopping criteria is met. The proposed algorithm can be summarized as follows:

  1. Generate an initial DOE $\mathbf{X}$ using an optimized *Latin hypercube*;
  1. Evaluate $\mathbf{X}$ using high-fidelity models and store responses of $\mathbf{f} = f_{(1 \ldots m)}$ and $\mathbf{g} = g_{(1   1. **while** computational budget not exhausted **do**:
    a. For each of the $m$-objectives and $p$-constraints, build a Kriging model;
    a. Generate a Pareto set $\mathbf{P}$ using the mean predictor of the Kriging models using a state-of-art multiobjective optimization algorithm (such as NSGA-II);
    a. Find $\mathbf{x}^\star \in \mathbf{P}$ that maximizes the \textsl{variance of the Kriging predictor}: $\mathbf{x}^\star = \text{arg}(\text{max}(\text{s}_\text{km}(\mathbf{x})))$;
    a. Evaluate the ``true'' values of $f(\mathbf{x}^\star$) and $g(\mathbf{x}^\star)$ using high-fidelity models and update the database.
  1. **end while**

Here, the NSGA-II implementation used is the one provided by the `R` package `mco` [@mco].

## The Nowacki Beam

However Kriging-based optimization is more useful for costly black box optimization problems, here we will demonstrate the technique using an analytic function for didactic proposes.

in the well known Nowacking beam optimization problem [@nowacki1980modelling], the aim is to design a tip loaded cantilever beam for minimum cross-sectional area and bending stress. The beam length is $l=1500\;\text{mm}$ and at is subject to a tip load force of $F=5000\;\text{N}$. The cross-section of the beam is rectangular, with breadth $b$ and height $h$, which are the design variables. The design is constrained by 5 requisites and the optimization problem can be formally defined as the following:

$$
\begin{align}
    \text{find:}            &\: \{b, h\}, \nonumber\\
    \text{where:}           &\:  \{20 \leq h \leq 250\}, \nonumber\\ 
    \text{and:}             &\: \{10 \leq b \leq 50\},  \nonumber\\ 
	\text{to minimize A:}   &\: A = b\,h \nonumber\\
	\text{and minimize B:}  &\: \sigma = \frac{6Fl}{b^2h}, \nonumber\\
	\text{subject to 1:}    &\: \delta = \frac{12Fl^3}{Ebh^3} \leq 5,  \\
	\text{subject to 2:}    &\: \sigma = \frac{6Fl}{b^2h} \leq 240,  \nonumber\\
	\text{subject to 3:}    &\: \tau = \frac{3F}{2bh} \leq 120,  \nonumber\\
	\text{subject to 4:}    &\: \text{AR} = \frac{h}{b} \leq 10,  \nonumber\\
	\text{subject to 5:}    &\: F_\text{crit} = -\frac{4}{l^2}\sqrt{G\frac{(b^3h+hb^3)}{12}E\frac{b^3h}{12}\frac{1}{(1-v^2)}} \leq -2, \nonumber
\end{align}
$$
where $E$ and $G$ are the longitudinal and the transverse Young's moduli of the material, respectively and $\nu$ is the Poisson's ration. All values are physically interpreted on the unit system $[\text{mm}$, $\text{N}$, $\text{MPa}]$. 





# References
